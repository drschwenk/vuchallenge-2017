<H3>Evaluation Criteria</H3>
For the action recognition competition, MAP (mean average precision) is used as the evaluation metric.
Separate leader boards are kept for performance on the competition's two subtasks.
<br>
<H4><b>Action Recognition</b></H4>
<p>
    For this task, participants should submit a zip file containing a csv file of the form:
</p>
    <pre>
        id &nbsp; vector
    </pre>
<p>
    where 'id' is a unique video id, and 'vector' is a whitespace delimited list of 157 floating point numbers representing the scores for each action class in a video.
    An example submission file is provided in the starter kit. The evaluation server calculates the mean average precision (mAP) for the videos. That is, the average of the average precision (AP) for a single activity in all the videos.
</p>

<H4><b>Temporal Segmentation</b></H4>
<p>
   For this task, the evaluation server requires a csv submission file of the form:
</p>
    <pre>
        id &nbsp; framenumber &nbsp; vector
    </pre>
<p>
    where 'id' is a unique video id, 'framenumber' is the number of the individual frame described below, and 'vector' is a whitespace delimited list of 157 floating point numbers representing the scores of each action in a frame.
    An example submission file is provided in the starter kit (download this file with get_test_submission_localize.sh).
</p>
<p>
    To avoid extremely large submission files, the evaluation script evaluates mAP on 25 equally spaced frames throughout each video. The frames are chosen as follows
</p>
<p>
</p>
    <pre>
        for j = 1, frames_per_video:
            timepoint(j) = (j-1)*video_duration/frames_per_video
    </pre>
<p>
    We use frames_per_video = 25, and so a sequence of timepoints for a video is:
</p>
    <pre>
    [0, video_duration/25, 2*video_duration/25, ..., 24*video_duration/25]
    </pre>
