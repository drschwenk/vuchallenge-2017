<H3>Evaluation Criteria</H3>
The submission are evaluated separately on the competition's two subtasks.
<H4><b>Action Recognition</b></H4>
<p>
    For this task, the evaluation server requires a csv file to be submitted in the following form:

    id vector
    where 'id' is a video id for a given video, and 'vector' is a whitespace delimited list of 157 floating point numbers representing the scores for each action in a video. An example submission file is provided in the starter kit.
    The evaluation server calculates the mean average precision (mAP) for the videos. That is, the average of the average precision (AP) for a single activity in all the videos.
</p>

<H4><b>Temporal Segmentation</b></H4>
<p>
   For this task, the evaluation server requires a csv submission file of the form:
</p>
<p>
    id &nbsp; framenumber &nbsp; vector
</p>
<p>
    where 'id' is a video id for a given video, 'framenumber' is the number of frame described below, and 'vector' is a whitespace delimited list of 157 floating point numbers representing the scores of each action in a frame.
    An example submission file is provided in the starter kit (download this file with get_test_submission_localize.sh).

    To avoid extremely large submission files, the evaluation script evaluates mAP on 25 equally spaced frames throughout each video. The frames are chosen as follows
</p>
<p>
    for j=1:frames_per_video
    timepoint(j) = (j-1)*time/frames_per_video;
</p>
<p>
    That is: 0, time/25, 2*time/25, ..., 24*time/25.
</p>
<p>
    The baseline performance was generated by calculating the action scores at 75 equally spaced frames in the video (our batchsize) and picking every third prediction.
</p>
