<H3>Textbook Question Answering Challenge 2017</H3>
<br>
<img src="http://vuchallenge.org/images/tqa_overview.png" width="600" alt="" >
<br>
<p>
	The TQA challenge encourages work on the task of Multi-Modal Machine Comprehension (M3C) task.
						The M3C task builds on the popular Visual Question Answering (VQA) and Machine Comprehension (MC)
						paradigms by framing question answering as a machine comprehension task, where the context needed to
						answer questions is provided and composed of both text and images. The dataset constructed to showcase
						 this task has been built from a middle school science curriculum that pairs a given question to a
						 limited span of knowledge needed to answer it. The phenomenon explained and tested on in middle school
						 science can be fairly complex, and many questions require more than a simple look-up. Complex parsing
						 of the source material and reasoning are often both required, as is connecting information provided jointly in
						text and diagrams. Our experiments have shown that extensions of the state-of-the-art methods from
						 MC and VQA perform poorly on this task. New ideas and techniques are needed to address the challenges
						 introduced by this dataset.
</p>
<p>
	At a high level, the M3C task is to read a multi-modal context along with a multi-modal question and provide an answer.
	VQA tasks typically require common sense knowledge to answer many questions, in addition to the image itself. In contrast, the
	this task is meant to be self-contained. All of the information needed to answer a question is included in the context.
</p>

