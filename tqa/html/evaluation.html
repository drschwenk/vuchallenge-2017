<H3>Evaluation Criteria</H3>
For the textbook question answering competition, accuracy is used as the evaluation metric.
Separate leader boards are kept for performance on text-only questions, as well as diagram questions.
<br>
<H4>Submission Format</H4>
<p>
	Participants should submit a zip file containing a single json file of this form:
</p>
<pre>
    {
	"Question_ID": "letter corresponding to answer choice"
    }
</pre>
<p>
	where the Question_ID is the global key of the question being answered, and the value is a lowercase letter
	denoting one of the multiple choice answer options for that question (e.g. a, b, c, ... ).
	The filename should contain the string "tqa" to be recognized by the evaluation server, but it is otherwise flexible.
</p>
<p>

</p>
<br>
<H4>Text only Question Answering</H4>
<p>
	Evaluation and submission for this subtask are identical to the diagram question task, except that only the text
	questions are scored. Questions requiring a diagram are not scored, and can be left out of the submission file
	if you would like to participate only in this subtask.
	Questions are separated by type in the dataset (nested under diagramQuestions and nonDiagramQuestions), and they also
	have global id prefixes to indicate an associated diagram (DQ_* and NDQ_*).
</p>
